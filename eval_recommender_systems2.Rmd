---
title: "Recommender System 2"
author: "Alain Kuiete"
date: "4/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# ----------------Evaluation of Recommender Systems-------------


## Import libraries and preprocess the dataset

```{r}
set.seed(1)
library(magrittr)
library(pander)
library(recommenderlab)
library(ggplot2)
library(tictoc)
```

```{r}
ratings <- read.csv('C:\\DATA612\\DATA612ASSINGMENTS-master\\ml-20m\\ratings.csv')
```

```{r}
ratings <- ratings[1:600000,]
ratings <- ratings[complete.cases(ratings),]
```

Convert to realRatingMatrix
```{r}
ratingsMatrix <- sparseMatrix(as.integer(ratings$userId), as.integer(ratings$movieId),  x = ratings$rating)
colnames(ratingsMatrix) <- levels(ratings$movieId)
rownames(ratingsMatrix) <- levels(ratings$userId)
mratings <- as(ratingsMatrix, "realRatingMatrix")
```


Explore
```{r}
mratings
```

Select Subset

```{r}
(ratingShort <- mratings[rowCounts(mratings) > 90, colCounts(mratings) > 60])
```




Check and Remove Empty Lines
```{r}
ratingShort <- mratings[, colCounts(mratings) > 60]
ratingShort <- ratingShort[rowCounts(ratingShort) > 90, ]
ratingShort
```

```{r}
(ratings_movies <- ratingShort[, colCounts(ratingShort) != 0])
```


```{r}

```

## Set up One  Model and  Evaluate

```{r}
n_fold <- 4
items_to_keep <- 15
rating_threshold <- 3
eval_sets <- evaluationScheme(data = ratings_movies,
                              method = "cross-validation",
                              k = n_fold,
                              given = items_to_keep,
                              goodRating = rating_threshold)
```



```{r}
model_to_evaluate <- "IBCF"
model_parameters <- NULL
```



```{r}
eval_recommender <- Recommender(data = getData(eval_sets, "train"),
                                method = model_to_evaluate,
                                parameter = model_parameters)
```



```{r}
items_to_recommend <- 10
```



```{r}
eval_prediction <- predict(object = eval_recommender,
                           newdata = getData(eval_sets, "known"),
                           n = items_to_recommend,
                           type = "ratings")
class(eval_prediction)
```

## Evaluation of Recomendation Accuracy per user

```{r}
qplot(rowCounts(eval_prediction)) +
  geom_histogram(binwidth = 10) +
  ggtitle("Distribution of movies per user")
```



```{r}
eval_accuracy <- calcPredictionAccuracy(
  x = eval_prediction,
  data = getData(eval_sets, "unknown"),
  byUser = TRUE)
```



```{r}
## head(eval_accuracy)
```



```{r}
pander(head(eval_accuracy))
```



```{r}
qplot(eval_accuracy[, "RMSE"]) +
  geom_histogram(binwidth = 0.1) +
  ggtitle("Distribution of the RMSE by user")
```

#### Overall Recommendation Accuracy

```{r}
eval_accuracy <- calcPredictionAccuracy(
  x = eval_prediction,
  data = getData(eval_sets, "unknown"),
  byUser = FALSE)
eval_accuracy
```



```{r}
results <- evaluate(x = eval_sets,
                    method = model_to_evaluate,
                    n = seq(10, 100, 10))
class(results)
```



```{r}
## head(getConfusionMatrix(results)[[1]])
```


```{r}
pander(head(getConfusionMatrix(results)[[1]]))
```


```{r}
columns_to_sum <- c("TP", "FP", "FN", "TN")
indices_summed <- Reduce("+", getConfusionMatrix(results))[, columns_to_sum]
```



```{r}
## head(indices_summed)
```


```{r}
pander(head(indices_summed))
```



```{r}
plot(results,
     annotate = TRUE,
     main = "ROC curve")
```



```{r}
plot(results, "prec/rec",
     annotate = TRUE,
     main = "Precision-recall")
```

## Comparison of Different Models of Recommender System

```{r}
library(pander)
set.seed(1)
library(recommenderlab)
library(ggplot2)

```

```{r}
data(MovieLense)
ratings_movies <- MovieLense[rowCounts(MovieLense) > 50,
                             colCounts(MovieLense) > 100]
```



```{r}
n_fold <- 4
items_to_keep <- 15
rating_threshold <- 3
eval_sets <- evaluationScheme(data = ratings_movies,
                              method = "cross-validation",
                              k = n_fold,
                              given = items_to_keep,
                              goodRating = rating_threshold)
```



```{r}
train <- getData(eval_sets, "train")
known <- getData(eval_sets, "known")
unknown <- getData(eval_sets, "unknown")
```


```{r}
## list(name = "IBCF", param = list(k = 20))
```



```{r}
models_to_evaluate <- list(
  IBCF_cos = list(name = "IBCF", param = list(method = "cosine")),
  IBCF_cor = list(name = "IBCF", param = list(method = "pearson")),
  UBCF_cos = list(name = "UBCF", param = list(method = "cosine")),
  UBCF_cor = list(name = "UBCF", param = list(method = "pearson")),
  SVD = list(name = "SVD", param = list(k = 50)),
  random = list(name = "RANDOM", param=NULL)
)
```



```{r}
n_recommendations <- c(1, 5, seq(10, 100, 10))
```


```{r}
list_results <- evaluate(x = eval_sets,
                    method = models_to_evaluate,
                    n = n_recommendations)
class(list_results)
```



```{r}
class(list_results[[1]])
```


```{r}
sapply(list_results, class) == "evaluationResults"
```



```{r}
avg_matrices <- lapply(list_results, avg)
```



```{r}
## head(avg_matrices$IBCF_cos[, 5:8])
```


```{r}
pander(head(avg_matrices$SVD)[, 5:8])
```



```{r}
plot(list_results,
     annotate = 1,
     legend = "topleft")
title("ROC curve")
```



```{r}
plot(list_results,
     "prec/rec",
     annotate = 1,
     legend = "bottomright")
title("Precision-recall")
```

 The models UBCF_cor and SVD perform better than other models. 
 We can make an hybrid model base on the UBCF_cor and SVD.  

 ## Hybrid Recommender Model
```{r}
model_method <- "SVD"

# Training
tic()
modelSVD <- Recommender(train, method = model_method, parameter = list(k = 50))
t <- toc(quiet = TRUE)
train_time <- round(t$toc - t$tic, 2)

# Predicting
tic()
predSVD <- predict(modelSVD, newdata = known, type = "ratings")
t <- toc(quiet = TRUE)
predict_time <- round(t$toc - t$tic, 2)

# timing <- rbind(timing, data.frame(Model = as.factor(model_method), Training = as.double(train_time), 
#     Predicting = as.double(predict_time)))

# Accuracy
accSVD <- calcPredictionAccuracy(predSVD, unknown)
# resultsSVD <- evaluate(x = eval, method = model_method, n = c(1, 5, 10,
# 30, 60))
```

```{r}
accSVD
```
 
 
```{r}
model_method <- "UBCF"
library(tictoc)
# Training
tic()
modelUBCF <- Recommender(train, method = model_method)
t <- toc(quiet = TRUE)
train_time <- round(t$toc - t$tic, 2)

# Predicting
tic()
predUBCF <- predict(modelUBCF, newdata = known, type = "ratings")
t <- toc(quiet = TRUE)
predict_time <- round(t$toc - t$tic, 2)

# timing <- rbind(timing, data.frame(Model = as.factor(model_method), Training = as.double(train_time), 
#     Predicting = as.double(predict_time)))
# Accuracy
accUBCF <- calcPredictionAccuracy(predUBCF, unknown)
resultsUBCF <- evaluate(x = eval_sets, method = model_method, n = c(1, 5, 10, 30, 60))
``` 
 
```{r}
accUBCF
```
 
 
 
```{r}
model_Hybrid <- HybridRecommender(modelUBCF, modelSVD, weights = c(0.99, 0.01))
pred_Hybrid <- predict(model_Hybrid, newdata = known, type = "ratings")
(accHybrid <- calcPredictionAccuracy(pred_Hybrid, unknown))
```


## Implement Support for Businessand User Experience Goal
It always possible to recommend a non popular product to a user and study the reaction of this user to the recommendation. This type of recommendation can improve user experience, expand user preference, and provide additional knowledge about a user.
Another approach to novelty/serendipity could be a weighted/hybrid solution of the top performing UBCF model, along with some randomness.

### Comparison of the accuracy

The accuracy really did not change. It may be better to use a model that chose the best model for a particular set of recommendation.

Let us look at top 10 recommendations for the first user in the test set.

```{r}
pUBCF <- predict(modelUBCF, newdata = known[1], type = "topNList")
pHybrid <- predict(model_Hybrid, newdata = known[1], type = "topNList")
```

```{r}
pUBCF@items
```

```{r}
pHybrid@items
```

Now as we see, the Hybrid model includes most of the items recommended by the UBCF model, but there are new items and the order is different.

##----------------------------- Conclusion-----------------------------------
. I used recommenderlab code and 20 million movie rating from movielens build my project.

â€¢ I first study the effect of IBCF on individual recommendatio. The recommenderlab allow me to build a list of six different recommender system algorithms and compared the accuracy of all the three different models. 

#### Reference:
Building a Recommendation System with R
Book by Michele Usuelli and Suresh K. Gorakala

DATA 612 Project 4 | Accuracy and Beyond 2019-07-01
http://rstudio-pubs-static.s3.amazonaws.com/509782_839ec319a8d24a70a9e2ddd0fefa02d7.html#objective

